---
title: Logistic Regression
author: Adogbeji Agberien
date: '2021-04-23'
slug: []
categories: []
tags: []
subtitle: ''
summary: 'This post is about Logistic Regression; currently just implementation in R, hopefully will soon update to include the theory of the method'
authors: []
lastmod: '2021-04-23T17:54:50-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>This post is about logistic regression; the theory and its implementation in R. Data from <a href="https://www.superdatascience.com/pages/machine-learning">superdatascience</a>.</p>
<pre class="r"><code># Install and load packages
required_packages &lt;- c(&quot;tidyverse&quot;, &quot;data.table&quot;, &quot;caTools&quot;,
                       &quot;rmarkdown&quot;, &quot;knitr&quot;)

packageCheck &lt;- lapply(required_packages, FUN = function(x) {
  if(!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})</code></pre>
<pre class="r"><code># Import the data 
social_network_ads &lt;- fread(&quot;C:/Users/diji_/Desktop/Data Science/Machine Learning A-Z (Codes and Datasets)/Part 3 - Classification/Section 14 - Logistic Regression/R/Social_Network_Ads.csv&quot;)</code></pre>
<p>Can we make predictions on whether or not an individual will purchase an item given Gender, Age, and Salary?</p>
<pre class="r"><code># Show the first few rows of the data
head(social_network_ads)</code></pre>
<pre><code>##     User ID Gender Age EstimatedSalary Purchased
## 1: 15624510   Male  19           19000         0
## 2: 15810944   Male  35           20000         0
## 3: 15668575 Female  26           43000         0
## 4: 15603246 Female  27           57000         0
## 5: 15804002   Male  19           76000         0
## 6: 15728773   Male  27           58000         0</code></pre>
<p>From the above, we see that we are given User ID, Gender, Age, Estimated Salary, and whether or not the individual made a purchase. The User ID intuitively wouldn’t have an effect of whether or not an individual purchases an item, so we do not need to include it in the model we make. Gender…well who knows it will have an effect…our model will inform us, but we have to encode the categorical variable before including it. We likely have to scale because variables Age will likely be “overshadowed” by Salary given their different ranges and variances.</p>
<pre class="r"><code># Encode Gender 
the_genders &lt;- unique(social_network_ads$Gender)
social_network_ads$Gender &lt;- factor(social_network_ads$Gender, levels = the_genders, labels = c(1, 2))</code></pre>
<pre class="r"><code># Split the data into training and test sets 
set.seed(123)
data_split &lt;- sample.split(social_network_ads$Purchased, SplitRatio = 0.8)
training_set &lt;- subset(social_network_ads, data_split == T)
test_set &lt;- subset(social_network_ads, data_split == F)</code></pre>
<pre class="r"><code>social_glm &lt;- glm(Purchased ~ EstimatedSalary + Age + Gender, data = training_set, family = binomial(link = &quot;logit&quot;))
summary(social_glm)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Purchased ~ EstimatedSalary + Age + Gender, family = binomial(link = &quot;logit&quot;), 
##     data = training_set)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.1148  -0.4856  -0.1091   0.3024   2.5188  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.396e+01  1.688e+00  -8.271  &lt; 2e-16 ***
## EstimatedSalary  4.159e-05  6.685e-06   6.221 4.94e-10 ***
## Age              2.660e-01  3.384e-02   7.861 3.82e-15 ***
## Gender2         -3.449e-01  3.525e-01  -0.979    0.328    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 416.79  on 319  degrees of freedom
## Residual deviance: 208.88  on 316  degrees of freedom
## AIC: 216.88
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>From the above we see that gender is deemed as insignificant with p-value of 0.328, so we exclude it from the model.</p>
<pre class="r"><code>social_glm_no_gender &lt;- glm(Purchased ~ EstimatedSalary + Age, data = training_set, family = binomial(link = &quot;logit&quot;))
summary(social_glm_no_gender)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Purchased ~ EstimatedSalary + Age, family = binomial(link = &quot;logit&quot;), 
##     data = training_set)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.1478  -0.5161  -0.1101   0.3106   2.4317  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.391e+01  1.673e+00  -8.316  &lt; 2e-16 ***
## EstimatedSalary  4.096e-05  6.619e-06   6.189 6.04e-10 ***
## Age              2.620e-01  3.322e-02   7.885 3.14e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 416.79  on 319  degrees of freedom
## Residual deviance: 209.84  on 317  degrees of freedom
## AIC: 215.84
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code># Make predictions on the test set
purchase_probability_test &lt;- predict(social_glm_no_gender, type = &quot;response&quot;, test_set)
purchase_prediction_test &lt;- if_else(purchase_probability_test &gt; 0.5, 1, 0)</code></pre>
<pre class="r"><code># Make a confusion matrix to evaluate prediction accuracy
confusion_matrix = table(purchase_prediction_test, test_set$Purchased)
confusion_matrix</code></pre>
<pre><code>##                         
## purchase_prediction_test  0  1
##                        0 44  8
##                        1  7 21</code></pre>
<p>From the confusion matrix above, we see that 44 individuals were correctly classified for not making purchases, and 21 were correctly classified for making purchases. 7 individuals were wrongly classified as having made a purchase when indeed they did not, and 8 individuals were also wrongly classified as not making purchased when indeed they made purchases. Thanks logistic regression.</p>
<p>Below we create a grid for aesthetically pleasing visualization of the purchase prediction.</p>
<pre class="r"><code># Create a grid for visualization
the_grid &lt;- expand.grid(seq(min(test_set$EstimatedSalary), max(test_set$EstimatedSalary), 100),
                        seq(min(test_set$Age), max(test_set$Age), 0.1)) %&gt;% data.table() %&gt;% 
  setnames(old = c(&quot;Var1&quot;, &quot;Var2&quot;), new = c(&quot;EstimatedSalary&quot;, &quot;Age&quot;))</code></pre>
<pre class="r"><code># Make a prediction on the grid data 
grid_prediction_prob &lt;- predict(social_glm_no_gender, type = &quot;response&quot;, the_grid)
grid_prediction &lt;- if_else(grid_prediction_prob &gt; 0.5, 1, 0)</code></pre>
<pre class="r"><code>ggplot() +
  geom_point(aes(the_grid$EstimatedSalary, the_grid$Age, colour = as.factor(grid_prediction), alpha = 0.3)) +
  geom_point(aes(test_set$EstimatedSalary, test_set$Age, colour = as.factor(test_set$Purchased))) +
  xlab(&quot;Salary&quot;) +
  ylab(&quot;Age&quot;) +
  ggtitle(&quot;Logistic Regression used to predict purchase of an item based on age and salary.&quot;) +
  theme_classic()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
